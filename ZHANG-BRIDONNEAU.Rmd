---
title: "project"
author: "Yue Zhang et Gérémi Bridonneau"
lang: fr
output:
  pdf_document:
    includes:
      in_header: preamble.tex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction



<!-------------------------------------- EX1 ------------------------------------------------------------------->
<!-------------------------------------- -- -------------------------------------------------------------------->

# 1 La transformation de Box-Cox

## 1. (réfléchir un nom/titre)

Si $\lambda = 0$, $h_{\lambda}(y) = log\ y$, $\forall y>0$.

On a $(1)$ $h_{\lambda}(y) = x^{'} \theta + \varepsilon$, $\varepsilon \sim \mathcal{N}(0, \sigma I_n)$.

-- (mon idée de solution)

La transformation $\tilde h_\lambda$ est valable seulement pour $y > 0$. De plus pour tout $\lambda \neq 0$, la transformation $\tilde h_\lambda$ est borné et donc la transformation ne peut pas être gaussienne. Pour $\lambda = 0$ on n'a pas ce problème grâce à la surjectivité du logarithme.

Si toute les observations sont positives on peut quand même utiliser cette transformation car on perdra qu'une faible partie des données normalement dans la queue à gauche de la répartition. Par exemple si les données ne suivent pas une loi normale mais une loi beta de paramètre $\alpha=2,\beta=2.2$ et qu'on utilise la transformation de Box et Cox avec $\lambda=2$ on obtient:

```{r}
lambda <- 2
a <- 2
b <- 2.2
plot({function (x) dbeta(x, a, b)})
plot({function (x) (dbeta(x, a, b)^lambda - 1)/lambda}, -0.2, 1.2)
```


On voit qu'on a aucune valeur négative et que donc la gaussianisation n'est pas parfaite mais cette transformation reste raisonnable.


## 2. Déterminer la fonction de vraisemblance

Supposons que pour $\beta = (\theta, \lambda, \sigma^2)^{'}$ a $p \times 1$ vecteur de paramètres, on ait $h_\lambda(Y_i) = Z_i = x_i \theta + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0,\sigma^2)$, $\varepsilon_i$ suivent une loi gaussien i.i.d. Donc par la définition de vraisemblance:

\begin{equation} \label{eq1}
\begin{split}
L(\lambda, \theta, \sigma^2; Y) & = \prod_{i=1}^{n} \pdv{F}{Y_i} \\
 & = \prod_{i=1}^{n} \dfrac{1}{\sqrt{2\pi\sigma^2}} \exp(-\dfrac{(h_{\lambda}(Y_i)-x_i\theta)^2}{2\sigma^2}) \left|\pdv{h_{\lambda}(Y_i)}{Y_i}\right| \\
 & = (\dfrac{1}{2\pi\sigma^2})^{\dfrac{n}{2}} \exp(- \dfrac{\sum_{i=1}^n (h_{\lambda}(Y_i)-x_i\theta)^2}{2\sigma^2}) \prod_{i=1}^{n} \left|\pdv{h_{\lambda}(Y_i)}{Y_i}\right| \\
 & = (\dfrac{1}{2\pi\sigma^2})^{\dfrac{n}{2}} \exp(- \dfrac{(h_{\lambda}(Y_i)-x_i\theta)^{'} (h_{\lambda}(Y_i)-x_i\theta)}{2\sigma^2}) \prod_{i=1}^{n} \left|Y_i^{\lambda-1}\right|
\end{split}
\end{equation}

Donc le terme $J(\lambda;Y) = \prod_{i=1}^{n} \left|\frac{\partial h_{\lambda}(Y_i)}{\partial Y_i}\right| = \prod_{i=1}^{n} \left|Y_i^{\lambda-1}\right|$, est la transformation de Jacobian de $h_{\lambda}(Y)-X\theta$ à $Y$.


## 3. Estimation du maximum de vraisemblance

A $\lambda$ fixé, on souhaite déterminer l'estimateur du maximum de vraisemblance $\widehat\theta(\lambda)$ et $\widehat\sigma^2(\lambda)$. Donc tout d'abord, depuis l'équation \ref{eq1} on calcule la log-vraisemblance.

\begin{equation} \label{eq2}
\begin{split}
\ell & = \log L(\lambda, \theta, \sigma^2; Y) \\
& = - \dfrac{n}{2}\log(2\pi\sigma^2) - \dfrac{(h_{\lambda}(Y)-X\theta)^{'} (h_{\lambda}(Y)-X\theta)}{2\sigma^2} + \sum_{i=1}^n \log \left|Y_i^{\lambda-1} \right| \\
& = -\dfrac{n}{2}\log(2\pi) -\dfrac{n}{2}\log(\sigma^2) -\dfrac{\norm{h_{\lambda}(Y)-X\theta}^2}{2\sigma^2} +(\lambda-1)\sum_{i=1}^n \log \left|Y_i \right| 
\end{split}
\end{equation}

Ensuite, étant donné que la log-vraisemblance $\ell$ l'équation \ref{eq2} est une transformation monotone de la vraisemblance $L$ dans l'équation \ref{eq1}, on maximise la log-vraisemblance $\ell$ respectivement pour $\theta$, $\sigma^2$ et $\lambda$, donc on obtient le premier ordre dérivation ci-dessous:

\begin{equation} \label{eq3}
\begin{split}
\pdv{\ell}{\sigma^2} & = -\dfrac{n}{2} \dfrac{1}{\sigma^2} + \dfrac{(h_{\lambda}(Y)-X\theta)^{'} (h_{\lambda}(Y)-X\theta)}{2\sigma^4} = 0
\end{split}
\end{equation}

Donc on a $\widehat{\sigma }^2 = \dfrac{(h_{\lambda}(Y)-X\theta)^{'} (h_{\lambda}(Y)-X\theta)}{n} = \dfrac{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)}{n}$, avec $H = X(X^{'}X)^{-1}X^{'}$ et $I_n$ matrice identité.

\begin{equation} \label{eq4}
\begin{split}
\pdv{\ell}{\theta} & = -\dfrac{2(-X)^{'}(h_{\lambda}(Y)-X\theta)}{2\sigma^2} \\
& = \dfrac{X^{'}(h_{\lambda}(Y)-X\theta)}{\sigma^2} = 0
\end{split}
\end{equation}

Donc, $\widehat{\theta} = (X^{'}X)^{-1}X^{'}h_{\lambda}(Y)$, par $X^{'}h_{\lambda}(Y) = X^{'}X\theta$.

Pour vérifier la formule avec $L_{max}(\lambda)$, on remplace nos emv $\widehat{\sigma}^2$ et $\widehat{\theta}$ calculés dans les équations \ref{eq3} et \ref{eq4} dans la log-vraisemblance $\ell$:

\begin{equation} \label{eq5}
\begin{split}
L_{max}(\lambda) & := \ell = \log L(\lambda, \widehat{\theta}(\lambda), \widehat{\sigma}^2(\lambda)) \\
 & = -\dfrac{n}{2}\log(\dfrac{\norm{h_{\lambda}(Y)-X\theta}^2}{n}) - \dfrac{\norm{h_{\lambda}(Y)-X\theta}^2 n}{2\norm{h_{\lambda}(Y)-X\theta}^2} + (\lambda-1)\sum_{i=1}^n \log\left|Y_i \right| - \dfrac{n}{2}\log(2\pi) \\
  & = -\dfrac{n}{2}\log(\widehat{\sigma}^2(\lambda)) + (\lambda-1)\sum_{i=1}^n \log\left|Y_i \right| -\dfrac{n}{2} -\dfrac{n}{2}\log(2\pi)
\end{split}
\end{equation}

Donc $a(n) = -\dfrac{n}{2} -\dfrac{n}{2}\log(2\pi)$ qui est bien une constante ne dépendant que de $n$. Maintenant on calcule l'emv $\widehat{\lambda}$:

\begin{equation} \label{eq6}
\begin{split}
\pdv{\ell}{\lambda} & = - \dfrac{2(h_{\lambda}(Y)-X\theta)\left|\pdv{h_{\lambda}(Y)}{\lambda} \right|}{2\sigma^2} + \sum_{i=1}^n \log\left|Y_i \right| = 0
\end{split}
\end{equation}

Et 

\begin{equation} \label{eq7}
\begin{split}
\pdv{L_{max}}{\lambda} & = -\dfrac{n}{2} \dfrac{1}{\widehat{\sigma}^2(\lambda)} \left|\pdv{\widehat{\sigma}^2(\lambda)}{\lambda} \right| + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -\dfrac{n}{2} \dfrac{1}{\widehat{\sigma}^2(\lambda)} \dfrac{2(h_{\lambda}(Y)-X\theta)\left|\pdv{h_{\lambda}(Y)}{\lambda} \right|}{n} + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -\dfrac{(h_{\lambda}(Y)-X\theta)\left|\pdv{h_{\lambda}(Y)}{\lambda} \right|}{\widehat{\sigma}^2}+ \sum_{i=1}^n \log\left|Y_i \right| = 0
\end{split}
\end{equation}

On peut bien vérifier que $\frac{\partial \ell}{\partial \lambda}$ et $\frac{\partial L_{max}}{\partial \lambda}$ sont équaux par calculation l'équation maximum vraisemblance. Par l'équation \ref{eq3}, on sait que $\widehat{\sigma}^2(\lambda) = \dfrac{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)}{n} = \dfrac{SCR(\lambda)}{n}$ avec $H = X(X^{'}X)^{-1}X^{'}$, est la somme de carrés résiduels de variance $h_{\lambda}(Y^)$ divisée par $n$. Depuis l'équation \ref{eq7}, on peut continuer cette calculation en remplaçant $\widehat{\sigma}^2$, et pour rappel $h_{\lambda}(Y) = \dfrac{Y^{\lambda}-1}{\lambda}$:

\begin{equation} \label{eq8}
\begin{split}
\pdv{L_{max}}{\lambda} & = -\dfrac{n}{2} \dfrac{n}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)} \dfrac{2h_{\lambda}(Y)^{'}(I_n - H)}{n} (\dfrac{Y^{\lambda}\log Y}{\lambda} - \dfrac{Y^{\lambda}-1}{\lambda^{2}}) + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -n \dfrac{h_{\lambda}(Y)^{'} (I_n-H)}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)} (\dfrac{Y^{\lambda}\log Y}{\lambda} - \dfrac{h_{\lambda}(Y)}{\lambda}) + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -n \dfrac{h_{\lambda}(Y)^{'} (I_n-H) \lambda^{-1}Y^{\lambda}\log Y}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)} + n \dfrac{ h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y) \lambda} + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -n \dfrac{h_{\lambda}(Y)^{'} (I_n-H) u_{\lambda}(Y)}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)} + \dfrac{n}{\lambda}+ \sum_{i=1}^n \log\left|Y_i \right| 
\end{split}
\end{equation}

avec $u_{\lambda}(Y) = \lambda^{-1}Y^{\lambda}\log Y$. Le numérateur dans l'équation \ref{eq8} est la somme résiduelle des produits dans l'analyse de la covariance de $h_{\lambda}(Y)$ et $u_{\lambda}(Y)$. Maintenant on utilise la transformation normalisée afin de simplifier le résultat, on définit $z_{\lambda}(Y)$ ci-dessous:


\begin{equation} \label{eq9}
\begin{split}
z_{\lambda}(Y) & = \dfrac{h_{\lambda}(Y)}{J(\lambda;Y)^{1/n}} \\
 & = \dfrac{h_{\lambda}(Y)}{(\prod_{i=1}^n |Y_i|)^{\lambda-1/n}}
\end{split}
\end{equation}

Donc $\widehat{\sigma}^2$ devient $\widehat{\sigma}^2(\lambda;z) = \dfrac{z_{\lambda}(Y)^{'} (I_n-H) z_{\lambda}(Y)}{n}= \dfrac{SCR(\lambda; z)}{n}$, $SCR(\lambda; z)$ est la somme des carrées résiduelle de $z_{\lambda}(Y)$. De plus, $L_{max} = -\dfrac{n}{2}\log(\widehat{\sigma}^2(\lambda;z)) + a(n)$, donc on propose de trouver $\widehat{\lambda}$ qui maximize $L_{max}(\lambda)$, c'est à dire minimize $\widehat{\sigma}^2(\lambda;z)$. Donc on cherche l'emc (estimateur des moindres carrées)

\begin{equation} \label{eq10}
\begin{split}
\widehat{\lambda} & = \argmin_{\lambda} SCR(\lambda; z)
\end{split}
\end{equation}


Pour répondre que l'emv est-il gaussien à distance finie? Je sais pas comment expliquer mais par le théorème du cours, l'emv est asymptotiquement normale non? emmm je vais réfléchir. genre la distribution de $\sqrt{n}(\widehat{\beta} - \beta)$, quand $n \rightarrow  \infty$, elle converge en une loi normale.


\begin{equation} \label{eq11}
\begin{split}
\left[\dfrac{I_n(\beta)^{-1}}{n} \right]^{1/2} \sqrt{n}(\widehat{\beta} - \beta) \rightarrow \mathcal{N}(0, I_n)
\end{split}
\end{equation}


$I_n(\beta)^{-1}$ est la matrice de l'information de Fisher.


## 4. Distribution asymptotique de l'emv

### Estimer la variance de $\widehat{\lambda}$

Soit $\widehat{\beta}$ l'emv asymptotiquement normal, 

\begin{equation} \label{eq12}
\begin{split}
\sqrt{n}(\widehat{\beta} - \beta) \rightarrow \mathcal{N}(0, I_n(\beta)^{-1}) \\
\widehat{\beta} \sim \mathcal{N}(\beta, I_n(\beta)^{-1})
\end{split}
\end{equation}

Par la définition, la matrice de l'information de Fisher est écrite ci-dessous:

\begin{equation} \label{eq13}
\begin{split}
I_n(\beta) & =\EX_{\beta}[\dot{\ell} \dot{\ell^{'}}] \\
 & = \EX_{\beta}[- \ddot{\ell}]
\end{split}
\end{equation}

où $\ddot{\ell}$ est la matrice Hessien $\ddot{\ell} = \dfrac{\partial^2 \ell}{\partial \beta \partial \beta^{'}}$ (pour rappelle que on a définit $\beta = (\theta, \lambda,  \sigma^2)^{'}$ a $p \times 1$ vecteur de paramètres). En particulier, on n'a pas forcément besoin d'estimer $\sigma^2$ simultanément avec $\theta$ et $\lambda$, donc pour simplifier les calculs, on décide de calculer la matrice Hessien de $L_{max}(\lambda)$. Dans l'équation \ref{eq7}, on a calculé $\dfrac{\partial L_{max}}{\partial \lambda} = -\dfrac{(h_{\lambda}(Y)-X\theta)\left|\dfrac{\partial h_{\lambda}(Y)}{\partial\lambda} \right|}{\widehat{\sigma}^2}+ \sum_{i=1}^n \log\left|Y_i \right|$, et on obtient sans souci $\dfrac{\partial L_{max}}{\partial \theta} = \dfrac{X^{'}(h_{\lambda}(Y)- X\theta)}{\widehat{\sigma}^2}$.

\begin{equation} \label{eq14}
\begin{split}
\ddot{\ell}:= H_n(\beta) & = \dfrac{\partial^2 L_{max}}{\partial \beta \partial \beta^{'}} \\
 & = - \widehat{\sigma}^{-2}
\begin{bmatrix}
X^{'}X & -X^{'} \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right| \\
- \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right| X & \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right|^{'} \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right| + \left|\dfrac{\partial^2 h_{\lambda}(Y)}{\partial^2 \lambda} \right| (h_{\lambda}(Y) - X\theta)
\end{bmatrix}
\end{split}
\end{equation}

$H_n(\beta)$ est bien une matrice définie négative. Etant donnée la distribution asymptotique normale de l'emv, on peut concluire que $\widehat{Var(\widehat\beta)} = - [H_n(\widehat{\beta})]^{-1}$, maintenant on calcul $\widehat{Var(\widehat \lambda)}$:

\begin{equation} \label{eq15}
\begin{split}
\widehat{Var(\widehat{\lambda)}} & = - H_n(\widehat \lambda)^{-1}
\end{split}
\end{equation}

où $H_n(\lambda) = \dfrac{\partial^2 L_{max}}{\partial^2 \lambda} = \dfrac{\left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right|^{'} \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right| + \left|\dfrac{\partial^2 h_{\lambda}(Y)}{\partial^2 \lambda} \right| h_{\lambda}(Y)^{'}(I_n-H)}{-\widehat \sigma^2}$.


### Intervalle de confiance

Etant donné que l'emv est asymptotiquement normalement distribué, donc on peut construire le test $T = \dfrac{\widehat{\beta} - \beta}{\sqrt{\dfrac{\widehat{Var(\widehat{\beta})}}{n}}} \sim \mathcal{N}(0, I_n)$. Par définition, $P(q_{\alpha/2} < \dfrac{\widehat{\beta}- \beta}{\sqrt{\dfrac{\widehat{Var(\widehat{\beta})}}{n}}} < q_{1-\alpha/2}) = 1-\alpha$, donc on peut obtenir l'intervalle de confiance $[\widehat{\beta} - q_{1-\alpha/2} \sqrt{\dfrac{\widehat{Var(\widehat{\beta})}}{n}}, \widehat{\beta} - q_{\alpha/2} \sqrt{\dfrac{\widehat{Var(\widehat{\beta})}}{n}}]$, où $q_{\alpha/2}$ et $q_{1-\alpha/2}$ sont quantiles d'ordre $\alpha/2$ et $1-\alpha/2$ sous la loi normale. La distribution est symétrique par rapport à 0, donc l'IC estimateur $\beta$ est également $[\widehat{\beta} - q_{1-\alpha/2} \sqrt{\dfrac{\widehat{Var(\widehat{\beta})}}{n}}, \widehat{\beta} + q_{1-\alpha/2} \sqrt{\dfrac{\widehat{Var(\widehat{\beta})}}{n}}]$.

L'intervalle de confiance de $\lambda$ donc est $[\widehat{\lambda} - q_{1-\alpha/2} \sqrt{\dfrac{\widehat{Var(\widehat{\lambda})}}{n}}, \widehat{\lambda} + q_{1-\alpha/2} \sqrt{\dfrac{\widehat{Var(\widehat{\lambda})}}{n}}]$, où $\widehat{Var(\widehat{\lambda})}$ est calculé dans l'équation \ref{eq15}.


### Test de Wald

On définit $A = [0, 1, 0]$, $\beta = (\theta, \lambda, \sigma^2)^{'}$, $\beta_0 = (\theta_0, \lambda_0, \sigma_0^2)^{'}$

$H_0: A(\beta - \beta_0) = 0$ contre $H_1: A(\beta-\beta_0) \neq 0$

Sous $H_0$, avec la delta méthode:

\begin{equation} \label{eq16}
\begin{split}
\sqrt{n}(A\widehat\beta - A\beta_0) \rightarrow \mathcal{N}(0, AV_nA^{'})
\end{split}
\end{equation}

\begin{equation} \label{eq17}
\begin{split}
T_n & = [AV_nA^{'}]^{-1/2}A(\widehat\beta - \beta_0) \rightarrow \mathcal{N}(0, 1)
\end{split}
\end{equation}

En utilisant la propriété de la statistique de Wald, $W$ est la carré de la norme de $T_n$ et sa loi asymptotique sous $H_0$ est:

\begin{equation} \label{eq18}
\begin{split}
W & = n(A\widehat\beta - A\beta_0)(A\widehat V_n A^{'})^{-1}(A\widehat\beta - A\beta_0)^{'} \rightarrow \chi^2(1)
\end{split}
\end{equation}

où $\widehat V_n = I_n(\widehat\beta)^{-1}$, et $W \geq 0$, la région de rejet est unilatère à droite de niveau asymptotique $\alpha$ pour une hypothèse bilatère est $\mathcal{R} = \left\{W > q_{1-\alpha}^{\chi^2(1)} \right\}$ avec $P_{(H_0)}(\mathcal{R}) \rightarrow \alpha$.


## 5. Test du rapport vraisemblance

Par le théorème asymptotique du $RV$, sous $H_0$:

\begin{equation} \label{eq19}
\begin{split}
TRV & = -2\log(RV) \rightarrow \chi^2(1)
\end{split}
\end{equation}

$TRV \geq 0$, la région de rejet $\mathcal{R} = \left\{TRV > q_{1-\alpha}^{\chi^2(1)} \right\}$ du test de rapport de vraisemblances maximales est asymptotiquement de niveau $\alpha$, $P_{(H_0)}(\mathcal{R}) \rightarrow \alpha$.

Par la définition de rapport de vraisemblance:

\begin{equation} \label{eq20}
\begin{split}
RV & = \dfrac{L(\lambda_0; Y)}{L(\widehat\lambda; Y)}
\end{split}
\end{equation}

où $L$ est la fonction de vraisemblance.

\begin{equation} \label{eq21}
\begin{split}
TRV & = -2\log (\dfrac{L(\lambda_0; Y)}{L(\widehat\lambda; Y)}) \\
 & = -2(\log L(\lambda_0; Y) - \log L(\widehat\lambda; Y)) \\
 & = 2(L_{max}(\widehat\lambda; Y) - L_{max}(\lambda; Y)) \\
 & = 2(-\dfrac{n}{2}\log(\widehat\sigma^2(\widehat\lambda)) + \dfrac{n}{2}\log(\widehat\sigma^2(\lambda))) \\
 & = n\log(\dfrac{\widehat\sigma^2(\lambda)}{\widehat\sigma^2(\widehat\lambda)})
\end{split}
\end{equation}


<!-------------------------------------- EX2 ------------------------------------------------------------------->
<!-------------------------------------- -- -------------------------------------------------------------------->

# 2 Test de la méthode sur des données simulées

```{r echo=FALSE}
lambda <- 0.3
a <- 5
b <- 1
variance <- 2
n=50
```

## 1. 

La condition de convergence indiquée dans la section 1 est-elle vérifiée?


```{r echo=FALSE}
set.seed(999)
X_obs <- rnorm(n)
Epsilon <- rnorm(n=50, mean=0, sd = sqrt(variance))


# Par la définition
Z <- t(a + b%*%X_obs + Epsilon)

# On considère que toutes les observations du jeu de données Y_i sont positives
Y <- (lambda*Z +1)^(1/lambda)

plot( sort(a + b%*%X_obs), sort(Z))
```


## 2.

```{r echo=FALSE}
X <- as.matrix(cbind(1, X_obs))
Q = diag(1,n) - X%*%solve(t(X)%*%X)%*%t(X)

tmp = rep(1, 50)
Lmle = function(Z){
  n = length(Z)
  sig2 = (t(Z)%*%Q%*%Z) / n
  -n/2 * log(sig2)
}

Lmin = function(lambda, Y) {
  -(Lmle(Z) + (lambda-1) * tmp%*%log(abs(Y)) - n/2)
}

lambda_x <- seq(0,2,0.05)
Vlmin = Vectorize(Lmin,"lambda")
plot(lambda_x, Vlmin(lambda_x, Y))



resopt = nlm(Lmin,Y=Y,p=2,hessian=TRUE)
```




<!-------------------------------------- EX3 ------------------------------------------------------------------->
<!-------------------------------------- -- -------------------------------------------------------------------->
# 3 Cas pratique





# Conclusion

