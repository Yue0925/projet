---
title: "project"
author: "Yue Zhang et Gérémi Bridonneau"
lang: "fr"
output: 
  pdf_document:
    includes:
      in_header: preamble.tex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE)
```

# Introduction

On va étudier une transformations non linéaires et en particulier la transformation de Box-Cox régulièrement utilisé pour stabiliser la variance et corriger les asymétries des données. On va dans un premier temps étudier théoriquement cette transformation et comment obtenir les paramètres optimaux de cette transformation. Dans un second temps on testera notre transformation sur des données simulées puis nous terminerons par un cas pratique sur l'étude du nombre de cycles à rupture d'un fil peigné en fonction de certains paramètres.

<!-------------------------------------- EX1 ------------------------------------------------------------------->
<!-------------------------------------- -- -------------------------------------------------------------------->

# 1 La transformation de Box-Cox

On s'intéresse au modèle d'observation $(x_i,Y_i)$:

\begin{equation}\label{mod} 
h_\lambda(Y_i)=Z_i=x_i\theta+\varepsilon_i,\;\varepsilon_i\stackrel{iid}\sim\mathcal N(0,\sigma^2)
\end{equation}

où $(h_\lambda)$ est une famille de transformations paramétrées par $\lambda$.

## Etude de la tranformation de Box-Cox.

Ici on s'intéresse à la transformation de Box-Cox définie par 

$$\forall \lambda \in \mathbb R, \forall y > 0, \tilde h_\lambda(y)=
   \begin{cases}
      \frac{y^\lambda -1}\lambda \;\lambda\neq 0\\
      \log y \;\lambda = 0
   \end{cases}$$

On remarque que théoriquement cette transformation est incompatible avec le modèle \ref{mod}. En effet la transformation $\tilde h_\lambda$ est valable seulement pour $y > 0$. De plus pour tout $\lambda \neq 0$, la transformation $\tilde h_\lambda$ est borné et donc la transformation ne peut pas être gaussienne. Pour $\lambda = 0$ on n'a pas ce problème grâce à la surjectivité du logarithme.

Si toute les observations sont positives on peut quand même utiliser cette transformation car on perdra qu'une faible partie des données normalement dans la queue à gauche de la répartition. Par exemple si les données ne suivent pas une loi normale mais une loi beta de paramètre $\alpha=2,\beta=2.2$ et qu'on utilise la transformation de Box et Cox avec $\lambda=2$ on obtient:

```{r echo=FALSE, include= TRUE}
lambda <- 2
a <- 2
b <- 2.2
plot({function (x) dbeta(x, a, b)}, main = "Loi beta de paramètres alpha=2, beta=2.2", ylab="f(y)")
plot({function (x) (dbeta(x, a, b)^lambda - 1)/lambda}, -0.2, 1.2, main = "La même loi beta après une transformation de Box et Cox avec lambda=2", ylab="h(f(y))")
```


On voit qu'on a aucune valeur négative et que donc la gaussianisation n'est pas parfaite mais cette transformation reste raisonnable.


## 2. Déterminer la fonction de vraisemblance

Supposons que pour $\beta = (\theta, \lambda, \sigma^2)^{'}$ a $p \times 1$ vecteur de paramètres, on ait $h_\lambda(Y_i) = Z_i = x_i \theta + \varepsilon_i, \; \varepsilon_i \sim \mathcal{N}(0,\sigma^2)$, $\varepsilon_i$ suivent une loi gaussien i.i.d. Donc par la définition de vraisemblance:

\begin{equation} \label{eq1}
\begin{split}
L(\lambda, \theta, \sigma^2; Y) & = \prod_{i=1}^{n} \pdv{F}{Y_i} \\
 & = \prod_{i=1}^{n} \dfrac{1}{\sqrt{2\pi\sigma^2}} \exp(-\dfrac{(h_{\lambda}(Y_i)-x_i\theta)^2}{2\sigma^2}) \left|\pdv{h_{\lambda}(Y_i)}{Y_i}\right| \\
 & = (\dfrac{1}{2\pi\sigma^2})^{\dfrac{n}{2}} \exp(- \dfrac{\sum_{i=1}^n (h_{\lambda}(Y_i)-x_i\theta)^2}{2\sigma^2}) \prod_{i=1}^{n} \left|\pdv{h_{\lambda}(Y_i)}{Y_i}\right| \\
 & = (\dfrac{1}{2\pi\sigma^2})^{\dfrac{n}{2}} \exp(- \dfrac{(h_{\lambda}(Y_i)-x_i\theta)^{'} (h_{\lambda}(Y_i)-x_i\theta)}{2\sigma^2}) \prod_{i=1}^{n} \left|Y_i^{\lambda-1}\right|
\end{split}
\end{equation}

Donc le terme $J(\lambda;Y) = \prod_{i=1}^{n} \left|\frac{\partial h_{\lambda}(Y_i)}{\partial Y_i}\right| = \prod_{i=1}^{n} \left|Y_i^{\lambda-1}\right|$, est la transformation de Jacobian de $h_{\lambda}(Y)-X\theta$ à $Y$.


## 3. Estimation du maximum de vraisemblance

A $\lambda$ fixé, on souhaite déterminer l'estimateur du maximum de vraisemblance $\widehat\theta(\lambda)$ et $\widehat\sigma^2(\lambda)$. Donc tout d'abord, depuis l'équation \ref{eq1} on calcule la log-vraisemblance.

\begin{equation} \label{eq2}
\begin{split}
\ell & = \log L(\lambda, \theta, \sigma^2; Y) \\
& = - \dfrac{n}{2}\log(2\pi\sigma^2) - \dfrac{(h_{\lambda}(Y)-X\theta)^{'} (h_{\lambda}(Y)-X\theta)}{2\sigma^2} + \sum_{i=1}^n \log \left|Y_i^{\lambda-1} \right| \\
& = -\dfrac{n}{2}\log(2\pi) -\dfrac{n}{2}\log(\sigma^2) -\dfrac{\norm{h_{\lambda}(Y)-X\theta}^2}{2\sigma^2} +(\lambda-1)\sum_{i=1}^n \log \left|Y_i \right| 
\end{split}
\end{equation}

Ensuite, étant donné que la log-vraisemblance $\ell$ l'équation (\ref{eq2}) est une transformation monotone de la vraisemblance $L$ dans l'équation (\ref{eq1}), on maximise la log-vraisemblance $\ell$ respectivement pour $\theta$, $\sigma^2$ et $\lambda$, donc on obtient le premier ordre dérivation ci-dessous:

\begin{equation} \label{eq3}
\begin{split}
\pdv{\ell}{\sigma^2} & = -\dfrac{n}{2} \dfrac{1}{\sigma^2} + \dfrac{(h_{\lambda}(Y)-X\theta)^{'} (h_{\lambda}(Y)-X\theta)}{2\sigma^4} = 0
\end{split}
\end{equation}

Donc on a $\widehat{\sigma }^2 = \dfrac{(h_{\lambda}(Y)-X\theta)^{'} (h_{\lambda}(Y)-X\theta)}{n} = \dfrac{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)}{n}$, avec $H = X(X^{'}X)^{-1}X^{'}$ et $I_n$ matrice identité.

\begin{equation} \label{eq4}
\begin{split}
\pdv{\ell}{\theta} & = -\dfrac{2(-X)^{'}(h_{\lambda}(Y)-X\theta)}{2\sigma^2} \\
& = \dfrac{X^{'}(h_{\lambda}(Y)-X\theta)}{\sigma^2} = 0
\end{split}
\end{equation}

Donc, $\widehat{\theta} = (X^{'}X)^{-1}X^{'}h_{\lambda}(Y)$, par $X^{'}h_{\lambda}(Y) = X^{'}X\theta$.

Pour vérifier la formule avec $L_{max}(\lambda)$, on remplace nos emv $\widehat{\sigma}^2$ et $\widehat{\theta}$ calculés dans les équations (\ref{eq3}) et (\ref{eq4}) dans la log-vraisemblance $\ell$:

\begin{equation} \label{eq5}
\begin{split}
L_{max}(\lambda) & := \ell = \log L(\lambda, \widehat{\theta}(\lambda), \widehat{\sigma}^2(\lambda)) \\
 & = -\dfrac{n}{2}\log(\dfrac{\norm{h_{\lambda}(Y)-X\theta}^2}{n}) - \dfrac{\norm{h_{\lambda}(Y)-X\theta}^2 n}{2\norm{h_{\lambda}(Y)-X\theta}^2} + (\lambda-1)\sum_{i=1}^n \log\left|Y_i \right| - \dfrac{n}{2}\log(2\pi) \\
  & = -\dfrac{n}{2}\log(\widehat{\sigma}^2(\lambda)) + (\lambda-1)\sum_{i=1}^n \log\left|Y_i \right| -\dfrac{n}{2} -\dfrac{n}{2}\log(2\pi)
\end{split}
\end{equation}

Donc $a(n) = -\dfrac{n}{2} -\dfrac{n}{2}\log(2\pi)$ qui est bien une constante ne dépendant que de $n$. Maintenant on calcule l'emv $\widehat{\lambda}$:

\begin{equation} \label{eq6}
\begin{split}
\pdv{\ell}{\lambda} & = - \dfrac{2(h_{\lambda}(Y)-X\theta)\left|\pdv{h_{\lambda}(Y)}{\lambda} \right|}{2\sigma^2} + \sum_{i=1}^n \log\left|Y_i \right| = 0
\end{split}
\end{equation}

Et 

\begin{equation} \label{eq7}
\begin{split}
\pdv{L_{max}}{\lambda} & = -\dfrac{n}{2} \dfrac{1}{\widehat{\sigma}^2(\lambda)} \left|\pdv{\widehat{\sigma}^2(\lambda)}{\lambda} \right| + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -\dfrac{n}{2} \dfrac{1}{\widehat{\sigma}^2(\lambda)} \dfrac{2(h_{\lambda}(Y)-X\theta)\left|\pdv{h_{\lambda}(Y)}{\lambda} \right|}{n} + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -\dfrac{(h_{\lambda}(Y)-X\theta)\left|\pdv{h_{\lambda}(Y)}{\lambda} \right|}{\widehat{\sigma}^2}+ \sum_{i=1}^n \log\left|Y_i \right| = 0
\end{split}
\end{equation}

On peut bien vérifier que $\frac{\partial \ell}{\partial \lambda}$ et $\frac{\partial L_{max}}{\partial \lambda}$ sont égaux par calcule de l'équation du maximum de vraisemblance. Par l'équation (\ref{eq3}), on sait que $\widehat{\sigma}^2(\lambda) = \dfrac{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)}{n} = \dfrac{SCR(\lambda)}{n}$ avec $H = X(X^{'}X)^{-1}X^{'}$, est la somme de carrés résiduels de variance $h_{\lambda}(Y^)$ divisée par $n$. Depuis l'équation (\ref{eq7}), on peut continuer ce calcul en remplaçant $\widehat{\sigma}^2$ avec pour rappel $h_{\lambda}(Y) = \dfrac{Y^{\lambda}-1}{\lambda}$:

\begin{equation} \label{eq8}
\begin{split}
\pdv{L_{max}}{\lambda} & = -\dfrac{n}{2} \dfrac{n}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)} \dfrac{2h_{\lambda}(Y)^{'}(I_n - H)}{n} (\dfrac{Y^{\lambda}\log Y}{\lambda} - \dfrac{Y^{\lambda}-1}{\lambda^{2}}) + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -n \dfrac{h_{\lambda}(Y)^{'} (I_n-H)}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)} (\dfrac{Y^{\lambda}\log Y}{\lambda} - \dfrac{h_{\lambda}(Y)}{\lambda}) + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -n \dfrac{h_{\lambda}(Y)^{'} (I_n-H) \lambda^{-1}Y^{\lambda}\log Y}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)} + n \dfrac{ h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y) \lambda} + \sum_{i=1}^n \log\left|Y_i \right| \\
 & = -n \dfrac{h_{\lambda}(Y)^{'} (I_n-H) u_{\lambda}(Y)}{h_{\lambda}(Y)^{'} (I_n-H) h_{\lambda}(Y)} + \dfrac{n}{\lambda}+ \sum_{i=1}^n \log\left|Y_i \right| 
\end{split}
\end{equation}

avec $u_{\lambda}(Y) = \lambda^{-1}Y^{\lambda}\log Y$. Le numérateur dans l'équation (\ref{eq8}) est la somme résiduelle des produits dans l'analyse de la covariance de $h_{\lambda}(Y)$ et $u_{\lambda}(Y)$. Maintenant on utilise la transformation normalisée afin de simplifier le résultat, on définit $z_{\lambda}(Y)$ ci-dessous:


\begin{equation} \label{eq9}
\begin{split}
z_{\lambda}(Y) & = \dfrac{h_{\lambda}(Y)}{J(\lambda;Y)^{1/n}} \\
 & = \dfrac{h_{\lambda}(Y)}{(\prod_{i=1}^n |Y_i|)^{\lambda-1/n}}
\end{split}
\end{equation}

Donc $\widehat{\sigma}^2$ devient $\widehat{\sigma}^2(\lambda;z) = \dfrac{z_{\lambda}(Y)^{'} (I_n-H) z_{\lambda}(Y)}{n}= \dfrac{SCR(\lambda; z)}{n}$, $SCR(\lambda; z)$ est la somme des carrées résiduelle de $z_{\lambda}(Y)$. De plus, $L_{max} = -\dfrac{n}{2}\log(\widehat{\sigma}^2(\lambda;z)) + a(n)$, donc on propose de trouver $\widehat{\lambda}$ qui maximize $L_{max}(\lambda)$, c'est à dire minimize $\widehat{\sigma}^2(\lambda;z)$. Donc on cherche l'emc (estimateur des moindres carrées)

\begin{equation} \label{eq10}
\begin{split}
\widehat{\lambda} & = \argmin_{\lambda} SCR(\lambda; z)
\end{split}
\end{equation}


Par le théorème du cours, l'emv est asymptotiquement normal, donc la distribution de $\sqrt{n}(\widehat{\beta} - \beta)$, quand $n \rightarrow  \infty$, elle converge en une loi normale.


\begin{equation} \label{eq11}
\begin{split}
\widehat V ^{-1/2} \sqrt{n}(\widehat{\beta} - \beta) \rightarrow \mathcal{N}(0, I_{dp})
\end{split}
\end{equation}


$I_1(\beta)^{-1}$ est la matrice de l'information de Fisher, noté que $\widehat V = I_1(\beta)^{-1}$ et $I_{dp}$ est la matrice identité de taille $p$. Quand $n \geq 30$, par le théorème $TCL$, $\widehat\beta$ tends vers un vecteur gaussien, donc à distance finie la distribution de $\widehat\beta$ approche la loi gaussienne.


## 4. Distribution asymptotique de l'emv

### Estimer la variance de $\widehat{\lambda}$

Par la propriété de l'emv, quand $\widehat \beta$ tend à devenir gaussien, on peut prendre pour loi approchée à distance finie la loi asymptotique

\begin{equation} \label{eq12}
\begin{split}
\widehat V^{-1/2} (\widehat{\beta} - \beta) \stackrel{appr}\sim \mathcal{N}(0, I_{dp}) \\
\widehat{\beta} \stackrel{appr}\sim \mathcal{N}(\beta, I_1(\beta)^{-1})
\end{split}
\end{equation}

Par la définition, la matrice de l'information de Fisher est écrite ci-dessous:

\begin{equation} \label{eq13}
\begin{split}
I_1(\beta) & =\EX_{\beta}[\dot{\ell} \dot{\ell^{'}}] \\
 & = - \EX_{\beta}[\ddot{\ell}]
\end{split}
\end{equation}

où $\ddot{\ell}$ est la matrice Hessienne $\ddot{\ell} = \dfrac{\partial^2 \ell}{\partial \beta \partial \beta^{'}}$ (pour rappelle on a définit $\beta = (\theta, \lambda,  \sigma^2)^{'}$ a $p \times 1$ vecteur de paramètres). En particulier, on n'a pas forcément besoin d'estimer $\sigma^2$ simultanément avec $\theta$ et $\lambda$, donc pour simplifier les calculs, on décide de calculer la matrice Hessienne de $L_{max}(\lambda)$. Dans l'équation (\ref{eq7}), on a calculé $\dfrac{\partial L_{max}}{\partial \lambda} = -\dfrac{(h_{\lambda}(Y)-X\theta)\left|\dfrac{\partial h_{\lambda}(Y)}{\partial\lambda} \right|}{\widehat{\sigma}^2}+ \sum_{i=1}^n \log\left|Y_i \right|$, et on obtient sans souci $\dfrac{\partial L_{max}}{\partial \theta} = \dfrac{X^{'}(h_{\lambda}(Y)- X\theta)}{\widehat{\sigma}^2}$.

\begin{equation} \label{eq14}
\begin{split}
\ddot{\ell}:= H(\beta) & = \dfrac{\partial^2 L_{max}}{\partial \beta \partial \beta^{'}} \\
 & = - \widehat{\sigma}^{-2}
\begin{bmatrix}
X^{'}X & -X^{'} \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right| \\
- \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right| X & \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right|^{'} \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right| + \left|\dfrac{\partial^2 h_{\lambda}(Y)}{\partial^2 \lambda} \right| (h_{\lambda}(Y) - X\theta)
\end{bmatrix}
\end{split}
\end{equation}

$H(\beta)$ est bien une matrice définie négative. Etant donnée la distribution asymptotique normale de l'emv, on peut conclure que $\widehat{Var(\widehat\beta)} = - [H(\widehat{\beta})]^{-1}$, maintenant on calcul $\widehat{Var(\widehat \lambda)}$:

\begin{equation} \label{eq15}
\begin{split}
\widehat{Var(\widehat{\lambda)}} & = - H(\widehat \lambda)^{-1}
\end{split}
\end{equation}

où $H(\lambda) = \dfrac{\partial^2 L_{max}}{\partial^2 \lambda} = \dfrac{\left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right|^{'} \left|\dfrac{\partial h_{\lambda}(Y)}{\partial \lambda} \right| + \left|\dfrac{\partial^2 h_{\lambda}(Y)}{\partial^2 \lambda} \right| h_{\lambda}(Y)^{'}(I_n-H)}{-\widehat \sigma^2}$.


### Intervalle de confiance

L'emv est asymptotiquement normalement distribué, par la propriété dans l'équation (\ref{eq12}), on peut construire le test $T = \dfrac{\widehat{\beta} - \beta}{\sqrt{\widehat{Var(\widehat{\beta})}}} \sim \mathcal{N}(0, I_{dp})$. Par définition, $P(q_{\alpha/2} < \dfrac{\widehat{\beta}- \beta}{\sqrt{\widehat{Var(\widehat{\beta})}}} < q_{1-\alpha/2}) = 1-\alpha$, donc on peut obtenir l'intervalle de confiance $[\widehat{\beta} - q_{1-\alpha/2} \sqrt{\widehat{Var(\widehat{\beta})}}, \widehat{\beta} - q_{\alpha/2} \sqrt{\widehat{Var(\widehat{\beta})}}]$, où $q_{\alpha/2}$ et $q_{1-\alpha/2}$ sont quantiles d'ordre $\alpha/2$ et $1-\alpha/2$ sous la loi normale $\mathcal{N}(0,1)$. La distribution est symétrique par rapport à $0$, donc l'IC estimateur de $\beta$ est également $[\widehat{\beta} - q_{1-\alpha/2} \sqrt{\widehat{Var(\widehat{\beta})}}, \widehat{\beta} + q_{1-\alpha/2} \sqrt{\widehat{Var(\widehat{\beta})}}]$.

L'intervalle de confiance de $\lambda$ est donc $[\widehat{\lambda} - q_{1-\alpha/2} \sqrt{\widehat{Var(\widehat{\lambda})}}, \widehat{\lambda} + q_{1-\alpha/2} \sqrt{\widehat{Var(\widehat{\lambda})}}]$, où $\widehat{Var(\widehat{\lambda})}$ est calculé dans l'équation (\ref{eq15}).


### Test de Wald

On définit $A = [0, 1, 0]$, $\beta = (\theta, \lambda, \sigma^2)^{'}$, $\beta_0 = (\theta_0, \lambda_0, \sigma_0^2)^{'}$

$H_0: A(\beta - \beta_0) = 0$ contre $H_1: A(\beta-\beta_0) \neq 0$

Sous $H_0$:

\begin{equation} \label{eq16}
\begin{split}
T & = [AVA^{'}]^{-1/2}A(\widehat\beta - \beta_0) \rightarrow \mathcal{N}(0, I_{dp})
\end{split}
\end{equation}

En utilisant la propriété de la statistique de Wald, $W$ est la carré de la norme de $T$ et sa loi asymptotique sous $H_0$ est:

\begin{equation} \label{eq17}
\begin{split}
W & = (A\widehat\beta - A\beta_0)(A\widehat V A^{'})^{-1}(A\widehat\beta - A\beta_0)^{'} \\
 & = \dfrac{(\widehat\lambda - \lambda_0)^2}{\widehat{Var(\widehat\lambda)}} \rightarrow \chi^2(1)
\end{split}
\end{equation}

où $\widehat V = I_1(\widehat\beta)^{-1}$, et $W \geq 0$, la région de rejet est unilatère à droite de niveau asymptotique $\alpha$ pour une hypothèse bilatère est $\mathcal{R} = \left\{W > q_{1-\alpha}^{\chi^2(1)} \right\}$ avec $P_{(H_0)}(\mathcal{R}) \rightarrow \alpha$.


## 5. Test du rapport vraisemblance

Par le théorème asymptotique du $RV$, sous $H_0$:

\begin{equation} \label{eq18}
\begin{split}
TRV & = -2\log(RV) \rightarrow \chi^2(1)
\end{split}
\end{equation}

$TRV \geq 0$, la région de rejet $\mathcal{R} = \left\{TRV > q_{1-\alpha}^{\chi^2(1)} \right\}$ du test de rapport de vraisemblances maximales est asymptotiquement de niveau $\alpha$, $P_{(H_0)}(\mathcal{R}) \rightarrow \alpha$.

Par la définition de rapport de vraisemblance:

\begin{equation} \label{eq19}
\begin{split}
RV & = \dfrac{L(\lambda_0; Y)}{L(\widehat\lambda; Y)}
\end{split}
\end{equation}

où $L$ est la fonction de vraisemblance.

\begin{equation} \label{eq20}
\begin{split}
TRV & = -2\log (\dfrac{L(\lambda_0; Y)}{L(\widehat\lambda; Y)}) \\
 & = -2(\log L(\lambda_0; Y) - \log L(\widehat\lambda; Y)) \\
 & = 2(L_{max}(\widehat\lambda; Y) - L_{max}(\lambda; Y)) \\
 & = 2(-\dfrac{n}{2}\log(\widehat\sigma^2(\widehat\lambda)) + \dfrac{n}{2}\log(\widehat\sigma^2(\lambda))) \\
 & = n\log(\dfrac{\widehat\sigma^2(\lambda)}{\widehat\sigma^2(\widehat\lambda)})
\end{split}
\end{equation}


<!-------------------------------------- EX2 ------------------------------------------------------------------->
<!-------------------------------------- -- -------------------------------------------------------------------->

# 2 Test de la méthode sur des données simulées

```{r echo=FALSE}
# Initialisation vars
lambda_ <- 0.3
a <- 5
b <- 1
variance <- 2
n=50
```

## 1. Modélisation la regression linéaire simple

### Condition convergence

La condition de convergence indiquée dans la section 1 est bien vérifiée.

En effet on a que si $x_1, x_2, \dots \stackrel{i.i.d}\sim \mathcal N(0, 1)$,

\begin{equation}\label{eq21}
\begin{split}
  \frac{X'X}n = \frac 1 n 
                \begin{bmatrix}
                  1   & 1   & \dots & 1  \\
                  x_1 & x_2 & \dots & x_n 
                \end{bmatrix}
                \begin{bmatrix}
                  1     & x_1  \\
                  1     & x_2  \\
                  \vdots & \vdots\\
                  1     & x_n  \\
                \end{bmatrix}
                = \begin{bmatrix}
                    1 & \frac 1 n \sum_{i=1}^n x_i\\
                    \frac 1 n \sum_{i=1}^n x_i & \frac 1 n \sum_{i=1}^n {x_i}^2\\
                  \end{bmatrix}
\end{split}
\end{equation}

On a de plus $\mathbb E[{x_i}^2] = \mathrm{Var}(x_i) + \mathbb E[x_i]^2 = 1$.
Ainsi d'après la loi des grands nombres on a $(X'X)/n$ qui converge vers la matrice identité de taille $2\times 2$ qui est bien définie positive.

### Estimation de la regression linéaire simple

Le modèle linéaire simple donc est $z_i = \mu + \theta x_i + \sigma\varepsilon_i$, $\varepsilon_i \sim \mathcal{N}(0, 1)$ i.i.d. Le plan d'expérience $X$ est en taille $[n,p]$ i.e. $[50 \times 2]$, et $\widehat\theta = (X^{'}X)^{-1}X^{'}Z$, $\widehat\sigma^2 = \dfrac{1}{n-p} ||Z-X\widehat\theta ||^2$.

Par la définition, $\dfrac{y_i^{\lambda}-1}{\lambda} = h_{\lambda}(y_i) = z_i$, $y_i = (\lambda z_i+1)^{1/\lambda}$.

```{r echo=FALSE, include= TRUE}
#Q1
set.seed(999)
X_obs <- rnorm(n) #loi gaussien centrée réduite
Epsilon <- rnorm(n=50, mean=0, sd = sqrt(variance))
# Par la définition
Z <- t(a + b%*%X_obs + Epsilon) #vector
# On considère que toutes les observations du jeu de données Y_i sont positives
Y <- (lambda_*Z +1)^(1/lambda_)
X <- as.matrix(cbind(1, X_obs))

# Estimer la regression linéair simple
theta_est = solve(t(X)%*%X)%*% t(X)%*%Z
p = ncol(X) #2
sigma_est = sqrt(sum( (X%*%theta_est -Z)^2 )/(n-p))

{plot(Z, (X%*%theta_est), xlab = "vars observées", ylab = "vars ajustées", main = "Pour Z")
abline(0,1)}

{plot(Y, ((X%*%theta_est)*lambda_+1)^{1/lambda_}, xlab = "vars observées", ylab = "vars ajustées", main = "Pour Y")
abline(0,1)}

```


En Traçant la profondeur ajustée en fonction de la profondeur observée, on peut observer que le modèle n'est pas bien ajustée, les points s’allongent autour de la première bissectrice, mais les ajustements parfois ne sont pas proches des valeurs observées, donc ce qui implique aussi les bruts résidus fortes.


### Etude les résidus

Par la définition, les résidus $\widehat\varepsilon = Z-X\widehat\theta$. Et les résidus studentisés donc est $t_i = \dfrac{\widehat\varepsilon_i}{\widehat\sigma \sqrt{1-h_{i,i}}}$ où $h_{i,i}$ sont les éléments diagonaux de $H = X(X^{'}X)^{-1}X^{'}$.


```{r echo=FALSE, include=TRUE}
#Etudier les résidus 
residus <- Z - X%*%theta_est
{plot(X%*%theta_est, residus, xlab = "vars ajustées", ylab = "les résidus",main="résidus bruts")
abline(0,0)
abline(2,0, col="red")
abline(-2,0, col="red")
}
```


On voit que les résidus bruts sont fortes parce que les valeurs observées ne sont pas elles-mêmes proches de 0 et estimées avec une petite précision. La plupart d'eux sont raisonnablement compris entre $-2$ et $2$ sauf quatres points, en respectant la règle empirique d'appartenance de $95\%$ des résidus à l'intervalle $-2$ et $2$.


## 2. Mise en oeuvre le calcul $\widehat\lambda$

```{r echo=FALSE}
#Q2
# Créer la matrice X du plan d'expérience
X <- as.matrix(cbind(1, X_obs))
# Interprétation les codes
Q = diag(1,n) - X%*%solve(t(X)%*%X)%*%t(X) 
Lmle = function(Z){
  n = length(Z)
  sig2 = (t(Z)%*%Q%*%Z) / n
  -n/2 * log(sig2)
}
```


La variable $Q$ est $(I_n-H)$, où $I_n$ est la matrice identidé de taille $50\times 50$, $H = X(X^{'}X)^{-1}X^{'}$. Le variable $sig2$ égale à $\dfrac{h_{\lambda}(Y)^{'}(I_n-H)h_{\lambda}(Y)}{n}$ qui est exactement $\widehat\sigma^2$ où on a démontré dans l'équation (\ref{eq3}). La fonction $Lmle$ retourne le terme $-\dfrac{n}{2}\log(\widehat\sigma^2)$.


```{r echo=FALSE, include=TRUE}
# Coder la fonction lmin(lambda, Y) qui calcule −Lmax.
vec1 = rep(1, 50)
Lmin = function(lambda, Y=Y) {
  Z_ <- ((Y^lambda)-1)/lambda
  -Lmle(Z_) + (1-lambda) * vec1%*%log(abs(Y)) + n/2 + (n/2)*log(2*pi)
}
# Tracer −Lmax
lambdas <- seq(0.01,2,0.01)
Vlmin = Vectorize(Lmin,"lambda")
plot(lambdas,Vlmin(lambdas, Y),
     main="-Lmax en fonction de lambda",
     ylab="",
     type="l",
     col="blue")

```


On voit bien que la fonction $-L_{max}$ est une fonction quadratique convexe, au près de $\lambda$ à 0.4, $-L_{max}$ atteint au minimum.


## 3. Calcul $\widehat\lambda$ et $\widehat{Var(\widehat\lambda)}$

```{r echo=FALSE, warning=FALSE}
resopt = nlm(Lmin,Y=Y,p=2,hessian=TRUE)
lambda_est <- resopt$estimate
var_lambda_est <- 1/resopt$hessian
```

Etant donné que $Z=h_{\lambda}(Y)$ est la transformation de $Y$, par la définition donc $\lambda \neq 0$, pour la Méthodes Newton, on commence la itération à partir de $2$.  En utilisant la fonction optimisation $nlm$, on obtient la valeur estimée $\widehat\lambda$ est $resopt\$estimate = 0.3817253$. Comme démontré dans l'équation (\ref{eq15}), la variance de $\widehat\lambda$ est l'inverse de hessian donc est $0.02948455$. (Pour la minimisation $-L_{max}$, la matrice hessian est définie positive)


## 4. Tests hypothèses

### Intervale de confiance

Comme on a montré dans le premier section, l'intervalle de confiance pour $\lambda$ est $[\widehat{\lambda} - q_{1-\alpha/2} \sqrt{\widehat{Var(\widehat{\lambda})}}, \widehat{\lambda} + q_{1-\alpha/2} \sqrt{\widehat{Var(\widehat{\lambda})}}]$ au niveau asymptotiquement $1-\alpha$. Ici on fixe $\alpha$ à $0.05$, donc $q_{1-\alpha/2}$ est $1.959964$ la quantile d'ordre $1-\alpha/2$ sous loi normale, donc on obtien l'IC $[0.04517863, 0.718272]$.

```{r message=FALSE, include=FALSE}
alpha <- 0.05
q <- qnorm(1-alpha/2)
lambda_est - q*sqrt(var_lambda_est) #lower bound
lambda_est + q*sqrt(var_lambda_est) #upper bound
```


### Test de Wald

```{r warning=FALSE, include=FALSE}
q_chi <- qchisq(1-alpha, df=1) #quantile
q_chi
lambda_0 <- c(1, 1/2, 0.3, 0)
lambda_0
W_obs <-  (lambda_est - lambda_0)^2 / var_lambda_est
W_obs
p_value <- pchisq(W_obs, df=1, lower.tail = FALSE)
p_value
p_value-alpha>0 # conserve H_0?
```

Das l'équation (\ref{eq17}), on a montré $W = \dfrac{(\widehat\lambda - \lambda_0)^2}{\widehat{Var(\widehat\lambda)}} \rightarrow \chi^2(1)$. Donc on veut tester $H_0 : \lambda = \lambda_0$, contre $H_1: \lambda \neq \lambda_0$ par la statistique de Wald pour les quatres cas ci-dessous:

1) Par la définition de $h_{\lambda}(Y)$, quand $\lambda = 1$, les données $Y$ ne nécessitent pas de transformation, donc dans le test $\lambda_0=1$. On a $W_{obs} < q_{1-\alpha}^{\chi^2}$ qui ne se trouve pas dans la région de rejet (unilartère à droite), et p-valeur $0.4909476286 > \alpha$, donc rejette $H_1$ et accèpte $H_0$ avec risque de seconde espèce inconnue.

2) De même raison, quand $\lambda = 0.5$, la transformation à appliquer aux observations est en racine carrée. Donc dans le test on veut $\lambda_0 = 0.5$. On a $W_{obs} < q_{1-\alpha}^{\chi^2}$ qui se trouve dans la région de rejet (unilartère à droite), et p-valeur $4.324529e-21 > \alpha$, donc rejette $H_0$ et accèpte $H_1$ avec risque de premier espèce $\alpha = 5\%$.

3) Quand $\lambda_0 = 3$, on a $W_{obs} < q_{1-\alpha}^{\chi^2}$ qui ne se trouve pas dans la région de rejet, et p-valeur $6.341116e-01 > \alpha$, donc rejette $H_1$ et accèpte $H_0$ avec risque de seconde espèce inconnue.

4) Quand $\lambda_0 = 0$, on a $W_{obs} > q_{1-\alpha}^{\chi^2}$ qui se trouve dans la région de rejet (unilartère à droite), et p-valeur $2.621087e-02 < \alpha$, donc rejette $H_0$ et accèpte $H_1$ avec risque de premier espèce $\alpha = 5\%$.


Bien que les quatres valeurs ne se trouvent pas dans l'IC que l'on a calculé, mais parmis 1, 2 et 0, $\lambda = 0.3$ est le plus proche que l'IC et le test de Wald l'accèpte.


## 5. Test de rapport de vraisemblance

```{r echo=FALSE, message=TRUE, include=TRUE}
lambda_0[4] <- 0.000001
for(i in 1:4){
  print(paste("lambda=", lambda_0[i]))
  TRV <- 2 *(Lmin(lambda_0[i], Y) - Lmin(lambda_est, Y))
  print(paste("TRV: ", TRV))
  p_value <- pchisq(TRV, df=1, lower.tail = FALSE)
  print(paste("p_value: ", p_value))
  print(paste("conserve H_0?", p_value-alpha>0)) # conserve H_0?
}

```

Par l'équation (\ref{eq20}), $TRV$ suit une loi $\chi^2(1)$, donc de même façon on calcule les $TRV$ observés et les p-valeurs. De même façon d'analyser les résultats calculés, les conclusions sont mêmes que ce qui donées par le test Wald dans question précédente.

## 6. Vérification par fonction "powerTransform"

Etant donné que l'on a toujours des problèmes sur l'installation du package "car", on l'exécute sur compilateur en ligne et voici dessous les résultats obtenus.

![powerTransform](powerTransform_online.jpg)

```{r include=FALSE}
################################# Merci de décommenter ci vous souhaitez l'exécuter ########################
#library('car')
#res <- powerTransform(Y~X, family="bcPower")
#summary(res)
```


On peut voir que l'estimateur de la puissance transformation appliquée sur $Y$ est le même valeur que l'on a obtenu par maximation le vraisemblance $L_{max}$ calculé par $nlm$ dans la question 3. Puis l'intervalle de confiance $[0.0452, 0.7183]$ est identique que l'on calcule dans question 4 au niveau asymptotique $\alpha =0.05$. Comme on a montré dans question 5, $TRV \sim \chi^2(1)$, pour $\lambda = 1$ et $\lambda = 0$, notre $TRV$ observés et les p-valeurs sont les même que les résultats donnés par fonction $powerTransform$.




<!-------------------------------------- EX3 ------------------------------------------------------------------->
<!-------------------------------------- -- -------------------------------------------------------------------->
# 3 Cas pratique

```{r include=FALSE}
# Lire les données, vérfier que le data.frame obtenu comporte 27 observations.
df = read.csv2("NbCycleRupture.csv")
head(df)
dim(df)
str(df) #structure de data frame
```

## 1. Modèle regression linéaire multiple 

### Analyse regression LM multiple

On définit le modèle linéaire multiple ($M1$) $y = \mu + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \sigma\varepsilon$, $\varepsilon \sim \mathcal{N}(0, I_n)$ i.i.d, on note que $\theta = [\mu, \beta_1, \beta_2, \beta_3]$.

```{r echo=FALSE, include= TRUE}
#M1
res1 <- lm(y~.,data=df) 
{plot(res1$fitted, df$y, xlab = "vars ajustées", ylab = "vars observées",main="M1:y~x1+x2+x3")
abline(0, 1)}

longueur <- 50*df$x1+300
amplitude <- df$x2+9
chargement <- df$x3*5 + 45
res2=lm(df$y~ longueur + amplitude +chargement)
{plot(res2$fitted, df$y, xlab = "vars ajustées", ylab = "vars observées",main="y~longueur+amplitude+chargement")
abline(0, 1)}
```

D'abord, on voit que le modèle pour les variables transformées $M1 : y = \mu + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \sigma\varepsilon$ n'est pas bien ajusté, comme le montre visuellement la figure ci-dessus: certains les points s’allongent prêsquement autour de la première bissectrice, mais les premiers données et les derniers données s'éloignent assez loin que la droite $y=x$, pour la plupart donées, les ajustements un peu plus proches des valeurs observées mais avec certain des bruits assez évidents.


Et puis par la définition dans l'énoncé, on obtient les valeurs pour les variables non transformées (longueur, amplitude et chargement). De même façon, on applique aussi modèle linéaire multiple, mais les deux ajustemnts sont identiques. Bien sûre que les estimateurs de coéfficient ($\widehat \theta$) changent, parce que les variables explicatives (i.e. la nouvelle matrice expériance $X=[1,$ longueur, amplitude , chargement$]$) changent. Mais les valeurs ajustées ne changent pas, donc la performance de modèle ne change pas non plus. Comme on peut vérifier par les données ci-dessous, les estimateurs changent mais le $R^2$ ne change pas.


```{r echo=FALSE, include= TRUE}
summary(res1)
summary(res2)
```

Donc ici on n'analyse que les valeurs pour $y\sim x_1+x_2+x_3$, pour $y\sim longueur+amplitude+chargement$ c'est la même façon et les conclusions sont les mêmes que pour $y\sim x_1+x_2+x_3$.

### Analyse la significativité des variables

Ici on veut tester chaque composante $\theta_i$ dans $\theta$. $H_0: \theta_i = 0$ contre $H_1: \theta_i \neq 0$, on suppose que $\alpha = 0.05$.

1) $\mu$: p-valeur $3.83e-09 < \alpha$, donc on rejette $H_0$ et accèpte $H_1$, c'est-à-dire on décide que le coéfficient $\mu$ est non nul avec risque de première espèce $\alpha = 0.05$, et l'intercept est utile dans $M1$ donc elle est sifnificative.

2) $\beta_1$: p-valeur $7.66e-06 < \alpha$, donc on rejette $H_0$ et accèpte $H_1$, c'est-à-dire on décide que le coéfficient $\beta_1$ est non nul avec risque de première espèce $\alpha = 0.05$, et $x_1$ est utile dans $M1$ donc elle est sifnificative.

3) $\beta_2$: p-valeur $0.000109 < \alpha$, donc on rejette $H_0$ et accèpte $H_1$, c'est-à-dire on décide que le coéfficient $\beta_2$ est non nul avec risque de première espèce $\alpha = 0.05$, et $x_2$ est utile dans $M1$ donc elle est sifnificative.

4) $\beta_3$: p-valeur $0.012734 < \alpha$, donc on rejette $H_0$ et accèpte $H_1$, c'est-à-dire on décide que le coéfficient $\beta_3$ est non nul avec risque de première espèce $\alpha = 0.05$, et $x_3$ est utile dans $M1$ donc elle est sifnificative.

En conclusion, on considère de garder tous les composantes variables dans $M1$.

### Analyse la significativité globale de la regression

Il s'agit de test Fisher sur un sous modèle linéaire du $M1$. On veut tester $H_0: \mu = \beta_1 = \beta_2 = \beta_3 = 0$, contre $H_1:$ l'un des paramètres n'est pas nul.

On observe que le F-statistique observé est $20.63$, et son p-valeur est $1.028e-06 < \alpha = 0.05$, donc on rejette $H_0$ et accèpte $H_1$, avec risque d'erreur de première espèce $\alpha$. On peut concluire que au moins un des quatres coéfficient (composante paramètre) n'est pas nul, au moins un des quatres variables (intercept et $x_1, x_2, x_3$) est signifigative.

### Analyse R squared

$R^2$ s'interprète comme la part de variance expliquée par les régresseurs supplémentaires. Donc $0.7291$ signifie que la population de $72.91\%$ de données peuvent être epliquées par notre modèle $M1$, la mauvaise qualité de l'ajustement et aussi énormément des brutes. En conclusion, $M1$ le LM multiple n'est pas un modèle idéel.


## 2. Modèle regression linéair d'ordre 2

### Etude redression LM d'ordre 2

Maintenant, on voudrais modéliser un modèle regression linéair d'ordre 2 $M2: y = \mu + \sum_i \beta_i x_i + \sum_{i} \beta_{ii} x_i^2 + \sum_{i<j} \beta_{i,j} x_i x_j + \varepsilon$. Voici dessous le graphe de l'ajustement:

```{r echo=FALSE, include=TRUE}
#M2
res3 <- lm(y~ df$x1 + df$x2 + df$x3 + I(df$x1^2) + I(df$x2^2) + I(df$x3^2) + I(df$x1*df$x2) + I(df$x1*df$x3) + I(df$x2*df$x3), data=df) 
{plot(res3$fitted, df$y, xlab = "vars ajustées", ylab = "vars observées",main="M2")
abline(0, 1)}
```

On voit bien visuellement que l'ajustement du modèle $M2$ est beaucoup plus amélioré que $M1$! Les points
s’allongent autour de la première bissectrice, les ajustements beaucoup proches des valeurs observées. En plus, il n'existe plus les valeurs négatives.

Maintenant, on analyse rapidement les valeurs donées par R de même façon que l'on a fait dans la question précédente.

```{r echo=FALSE, include=TRUE}
summary(res3)
```

### Analyse la significativité des variables

Ici on veut tester chaque composante $\theta_i$ dans $\theta$. $H_0: \theta_i = 0$ contre $H_1: \theta_i \neq 0$, on suppose que $\alpha = 0.05$. Etant donné que y'a dix composantes paramètres dans l'estimateur $\theta$, on n'analyse pas un par un variables ici.

En vérifiant chaque p-valeur correspondantes les variables, on peut concluire que seulement la variable $x_3^2$ et la variable $x_2x_3$ ne sont pas signifigatives. Parce que ses p-valeur sont supérieure à $\alpha$ donc on regarde $H_0$ (avec risque de seconde espèce inconnue) et décide que ses coéfficients sont nuls, donc les deux variables ne sont pas utiles dans $M2$. Pour la reste, ils sont tous signifigatives. En conclusion, on considère de ne pas garder $x_3^2$ et $x_2x_3$ dans $M2$.

### Analyse la significativité globale de la regression

On veut tester $H_0:$ tous les composantes de l'estimateur sont équalent à 0, contre $H_1:$ l'un des paramètres n'est pas nul.

On observe que le F-statistique observé est $28.51$, et son p-valeur est $1.564e-08 < \alpha = 0.05$, donc on rejette $H_0$ et accèpte $H_1$, avec risque d'erreur de première espèce $\alpha$. On peut concluire que au moins un des dix coéfficients (composantes paramètre) n'est pas nul, au moins un des dix variables est signifigative.

### Analyse R squared

$R^2$ s'interprète comme la part de variance expliquée par les régresseurs supplémentaires. Donc $0.9379$ signifie que la population de $93.79\%$ de données sont epliquées par notre modèle $M2$, ce qui implique un très bonne qualité de l'ajustement. En conclusion, $M2$ le LM multiple d'ordre 2 est un modèle assez performante.

### Test modèle

On veut consrtuire un test $M1$ contre $M2$ en utilisant "anova" fonction. En théorie, le F statistique est le même que le test de signifigativité globale de la regression. On voit bien le $F_{obs} = 9.5227$, p-valeur égale à $0.000115 < \alpha=0.05$, donc on conclut que les variables d'interaction ajoutées sont significatives.


```{r echo=FALSE, include=TRUE}
anova(res1, res3)
```

## 3. Test de variance


# Conclusion


```{r}
#library(knitr)
#purl("ZHANG-BRIDONNEAU.Rmd")
```

